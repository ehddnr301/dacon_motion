{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "motion-pytorch-hrnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMqkBEqH4ZnSVauWVQWbxJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehddnr301/dacon_motion/blob/master/motion_pytorch_hrnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVoal93KFfoj",
        "outputId": "684df844-6169-45e7-91ce-28b4eac3a4d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNBL_Y_whmjJ"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7ZQ-bhBKhdD"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhnwXQcjAS_Z"
      },
      "source": [
        "!rm -rf /content/gdrive/MyDrive/dacon-motion/train_imgs_folder"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpMUrxahXk2f"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-KQA_pAATFh"
      },
      "source": [
        "!rm -rf /content/gdrive/MyDrive/dacon-motion/test_imgs_folder"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPbzFiFJGodu",
        "outputId": "d17dac4f-ee7e-48b5-946c-612b52098b4c"
      },
      "source": [
        "!pip install torchinfo"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/b1/4b310bd715885636e7174b4b52817202fff0ae3609ca2bfb17f28e33e0a1/torchinfo-0.0.8-py3-none-any.whl\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-0.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g_UN8sb_3H2",
        "outputId": "8b7ebd5c-fd87-4594-d649-39facd47ed46"
      },
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
            "\u001b[K     |██████████████████████████████▊ | 112kB 803kB/s eta 0:00:01"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVYs0ebF4hFo"
      },
      "source": [
        "!pip install -U albumentations_experimental"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFicxXrGHqim"
      },
      "source": [
        "!git clone https://github.com/ehddnr301/pytorch-image-models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOJ60JwkFlbP"
      },
      "source": [
        "!mv pytorch-image-models timmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LygSesr6F259"
      },
      "source": [
        "!cd timmm && pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSu8WWhpFldj"
      },
      "source": [
        "from timmm import timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IYKcH9a_Lav"
      },
      "source": [
        "!unzip -n '/content/gdrive/MyDrive/dacon-motion/train_imgs.zip' -d '/content/gdrive/MyDrive/dacon-motion/train_imgs_folder'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9sp95hX_Mk_"
      },
      "source": [
        "!unzip -n '/content/gdrive/MyDrive/dacon-motion/test_imgs.zip' -d '/content/gdrive/MyDrive/dacon-motion/test_imgs_folder'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw7ZEgkxF4ke"
      },
      "source": [
        "import os\n",
        "from typing import Tuple, Sequence, Callable\n",
        "import csv\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchinfo import summary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import albumentations\n",
        "from albumentations.pytorch import ToTensorV2, ToTensor\n",
        "from albumentations_experimental import HorizontalFlipSymmetricKeypoints\n",
        "\n",
        "from torchvision import transforms\n",
        "import timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy9qTTh33g7L"
      },
      "source": [
        "# random seed\n",
        "random_seed = 301\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "\n",
        "IMAGE_HEIGHT = 1024\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_CHANNELS=3\n",
        "EPOCHS=30\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "PATH_TRAIN_DATASET='/content/gdrive/MyDrive/dacon-motion/train_imgs_folder/'\n",
        "PATH_TEST_DATASET='/content/gdrive/MyDrive/dacon-motion/test_imgs_folder/'\n",
        "PATH_TRAIN_ANS_CSV='/content/gdrive/MyDrive/dacon-motion/train_df.csv'\n",
        "SUB_DF = '/content/gdrive/MyDrive/dacon-motion/sample_submission.csv'\n",
        "PATH_TRAIN_CSV='/content/gdrive/MyDrive/dacon-motion/split_train_df.csv'\n",
        "PATH_VALID_CSV='/content/gdrive/MyDrive/dacon-motion/split_val_df.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xukO8OtMecSQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qfcdyqi4NTP"
      },
      "source": [
        "df = pd.read_csv(PATH_TRAIN_ANS_CSV)\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBHnZanxjVDJ"
      },
      "source": [
        "error_list=[317, 869, 873, 877, 911, 1559, 1560, 1562, 1566, 1575, 1577, 1578, 1582, 1606, 1607, 1622, 1623, 1624, 1625, 1629, 3968, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WuDBvZkjVNc"
      },
      "source": [
        "df.drop(error_list, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkI7YWbQ-kYh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3NKd6Aej8Yf"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wWJ2u4tsOH7"
      },
      "source": [
        "x_coor = [col for col in df.columns if '_x' in col]\n",
        "y_coor = [col for col in df.columns if '_y' in col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3UeCdVE1Axq"
      },
      "source": [
        "val_name_list = []\n",
        "check_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6gx5X55kInR"
      },
      "source": [
        "def spliting(x):\n",
        "  x1 = x.split('000')[0]\n",
        "  if check_list.count(x1) < 1:\n",
        "    check_list.append(x1)\n",
        "    val_name_list.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk8B3ZazkIpz"
      },
      "source": [
        "df['image'].apply(spliting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdUzjwX_ZsVt"
      },
      "source": [
        "val_name_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5bLzYomkIsb"
      },
      "source": [
        "val_df = df[df['image'].isin(val_name_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOG2ElFrl3_y"
      },
      "source": [
        "train_df = df[~df['image'].isin(val_name_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8n-ylW4lkdq"
      },
      "source": [
        "val_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaq68269l-kA"
      },
      "source": [
        "train_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJlehx0p1G5E"
      },
      "source": [
        "train_imgs = train_df.iloc[:, 0].to_numpy()\n",
        "motions = train_df.iloc[:, 1:]\n",
        "columns = motions.columns.to_list()[::2]\n",
        "class_labels = [label.replace('_x', '').replace('_y', '') for label in columns]\n",
        "train_keypoints = []\n",
        "for motion in motions.to_numpy():\n",
        "    a_train_keypoints = []\n",
        "    for i in range(0, motion.shape[0], 2):\n",
        "        a_train_keypoints.append((float(motion[i]), float(motion[i+1])))\n",
        "    train_keypoints.append(a_train_keypoints)\n",
        "train_keypoints = np.array(train_keypoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG47t2HomjlW"
      },
      "source": [
        "val_imgs = val_df.iloc[:, 0].to_numpy()\n",
        "motions = val_df.iloc[:, 1:]\n",
        "columns = motions.columns.to_list()[::2]\n",
        "class_labels = [label.replace('_x', '').replace('_y', '') for label in columns]\n",
        "val_keypoints = []\n",
        "for motion in motions.to_numpy():\n",
        "    a_val_keypoints = []\n",
        "    for i in range(0, motion.shape[0], 2):\n",
        "        a_val_keypoints.append((float(motion[i]), float(motion[i+1])))\n",
        "    val_keypoints.append(a_val_keypoints)\n",
        "val_keypoints = np.array(val_keypoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jleMgT551XqC"
      },
      "source": [
        "len(val_keypoints), len(val_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AegQ41Q-m2Ef"
      },
      "source": [
        "len(train_keypoints), len(train_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXjFbT85DBC"
      },
      "source": [
        "class_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SidE89LGg_rE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyX13YJBn3Gu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2g0s_YY4J7n"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train, valid = train_test_split(df, test_size=0.05, random_state=random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YOxEjGt4ghU"
      },
      "source": [
        "class MotionDataset(Dataset):\n",
        "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
        "    def __init__(self, data_dir, imgs, keypoints, class_labels=None, data_transforms=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.imgs = imgs\n",
        "        self.keypoints = keypoints\n",
        "        self.class_labels = class_labels\n",
        "        self.data_transforms = data_transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Read an image with OpenCV\n",
        "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
        "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        keypoints = self.keypoints[idx]\n",
        "    \n",
        "        if self.data_transforms:\n",
        "            augmented = self.data_transforms(image=img, keypoints=keypoints, class_labels=self.class_labels)\n",
        "            img = augmented['image']\n",
        "            keypoints = augmented['keypoints']\n",
        "            keypoints = [(x/1024, y/1024) for x,y in keypoints]\n",
        "            # keypoints = [(x, y) for x,y in keypoints]\n",
        "\n",
        "        keypoints = np.array(keypoints).flatten()\n",
        "\n",
        "        return img, keypoints\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb5qHSfNZl_J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85p0gTzQRV5M"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
        "    def __init__(self, data_dir, imgs, data_transforms=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.imgs = imgs\n",
        "        self.data_transforms = data_transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.imgs[idx]\n",
        "        # Read an image with OpenCV\n",
        "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
        "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.data_transforms:\n",
        "            augmented = self.data_transforms(image=img)\n",
        "            img = augmented['image']\n",
        "\n",
        "        return filename, img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWv4aeG-qJTb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU3iz9CO4gkY"
      },
      "source": [
        "transforms_train = albumentations.Compose([\n",
        "      albumentations.Crop(426, 56, 1450, 1080),\n",
        "      albumentations.OneOf([\n",
        "                HorizontalFlipSymmetricKeypoints(symmetric_keypoints={\n",
        "                    (0,0),(1,2),(3,4),(5,6),(7,8),(9,10),(11,12),(13,14),(15,16),(17,17),(18,19),(20,20), (21,21),(22,23)\n",
        "                }, p=1),\n",
        "                albumentations.Rotate(limit=45, p=1),\n",
        "        ], p=0.5),\n",
        "      albumentations.OneOf([                    \n",
        "          albumentations.RandomBrightness(p=1),\n",
        "          albumentations.MotionBlur(blur_limit=[3,20], p=0.5),\n",
        "      ], p=0.5),\n",
        "      albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "      ToTensorV2()\n",
        "], keypoint_params=albumentations.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=False))\n",
        "\n",
        "transforms_valid = albumentations.Compose([\n",
        "    albumentations.Crop(426, 56, 1450, 1080),\n",
        "    albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "], keypoint_params=albumentations.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=False))\n",
        "\n",
        "transforms_test = albumentations.Compose([\n",
        "    albumentations.Crop(426, 56, 1450, 1080),\n",
        "    albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpYqVyBHixW8"
      },
      "source": [
        "# trainset = MotionDataset(PATH_TRAIN_DATASET, PATH_TRAIN_CSV, transforms_train)\n",
        "# valset = MotionDataset(PATH_TRAIN_DATASET, PATH_VALID_CSV, transforms_valid)\n",
        "# testset = MotionDataset(PATH_TEST_DATASET, SUB_DF, transforms_test)\n",
        "test_imgs = os.listdir(PATH_TEST_DATASET)\n",
        "trainset = MotionDataset(PATH_TRAIN_DATASET, train_imgs, train_keypoints, data_transforms=transforms_train, class_labels=class_labels)\n",
        "valset = MotionDataset(PATH_TRAIN_DATASET, val_imgs, val_keypoints, data_transforms=transforms_valid, class_labels=class_labels)\n",
        "testset = TestDataset(PATH_TEST_DATASET, test_imgs, data_transforms=transforms_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=4)\n",
        "val_loader = DataLoader(valset, batch_size=1)\n",
        "test_loader = DataLoader(testset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31mpDdOvBLl2"
      },
      "source": [
        "# sample, keypointss = trainset[0]\n",
        "sample, _ = trainset[205]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8dSe-L1Vk3i"
      },
      "source": [
        "\n",
        "# keypointss = keypointss * 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrS3_zfWtG_J"
      },
      "source": [
        "def custom_imshow(img):\n",
        "  img = img.numpy()\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "  plt.show()\n",
        "# def custom_imshow(img, keypointss):\n",
        "#   print(keypointss)\n",
        "#   img = img.numpy()\n",
        "#   plt.figure(figsize=(12,12))\n",
        "#   for j in range(0,len(keypointss),2):\n",
        "#     plt.plot(keypointss[j], keypointss[j+1],'rx')\n",
        "#   plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFJO_NJUjR3o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2V2kGdstQdW"
      },
      "source": [
        "# custom_imshow(sample,keypointss)\n",
        "custom_imshow(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkthEFeyGlIh"
      },
      "source": [
        "class HRnetModel(nn.Module):\n",
        "    def __init__(self, num_classes=48, model_name='hrnet_w32', pretrained=True):\n",
        "        super(HRnetModel, self).__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
        "        self.model.classifier = nn.Linear(2048, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1LDc0VS-Eps"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HRnetModel().to(device)\n",
        "\n",
        "# 16번째 이후에 끊겨서 8epoch부터 다시 불러서 학습\n",
        "# PATH = '/content/gdrive/MyDrive/dacon-motion/checkpoint/model_hrnet_0321_16.pt'\n",
        "# model = HRnetModel().to(device)\n",
        "\n",
        "# checkpoint = torch.load(PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zl2hXPoRiXI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJYmiKwUg5kl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhGf1RHmHkmp"
      },
      "source": [
        "print(summary(model, input_size=(1, 3, 1024, 1024), verbose=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF8V1mMu-wXz"
      },
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# criterion = nn.MSELoss(reduction='sum')\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                  mode='min',\n",
        "                                                  factor=0.5,\n",
        "                                                  patience=2,)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "\n",
        "    for i, (images, targets) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # images = images.float().to(device)\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        model.train()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.float(), targets.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            outputs = outputs\n",
        "            \n",
        "            print(f'{epoch+1}: {loss.item():.5f}')\n",
        "\n",
        "\n",
        "    with tqdm(val_loader,\n",
        "            total=val_loader.__len__(),\n",
        "            unit=\"batch\") as valid_bar:\n",
        "        for i, (images, targets) in enumerate(valid_bar):\n",
        "            valid_bar.set_description(f\"Valid Epoch {epoch + 1}\")\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # 모델의 dropoupt, batchnormalization를 eval모드로 설정\n",
        "            # .forward()에서 중간 노드의 gradient를 계산\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # validation loss만을 계산\n",
        "                probs  = model(images)\n",
        "                valid_loss = criterion(probs.float(), targets.float())\n",
        "\n",
        "            valid_bar.set_postfix(valid_loss = valid_loss.item())\n",
        "\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, f'/content/gdrive/MyDrive/dacon-motion/checkpoint/model_hrnet_0326_{epoch + 1}.pt')\n",
        "    \n",
        "    print('------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kruZMuZaHMu0"
      },
      "source": [
        "# # 불러와서 예측하기\n",
        "\n",
        "# PATH = '/content/gdrive/MyDrive/dacon-motion/checkpoint/model_hrnet_0325_16.pt'\n",
        "# model = HRnetModel().to(device)\n",
        "\n",
        "# checkpoint = torch.load(PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NawUct-9KZj5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPd5dyaEUjk5"
      },
      "source": [
        "all_predictions = []\n",
        "files = []\n",
        "with torch.no_grad():\n",
        "    for filenames, inputs in tqdm(test_loader):\n",
        "        predictions = list(model(inputs.to(device)).cpu().numpy())\n",
        "        files.extend(filenames)\n",
        "        for prediction in predictions:\n",
        "            all_predictions.append(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTehYyVicWIn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us5hRDNeVPT2"
      },
      "source": [
        "all_predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3KpkWQpI2zq"
      },
      "source": [
        "all_predictions[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB3IAoIfVesl"
      },
      "source": [
        "files[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq673miTUqG-"
      },
      "source": [
        "all_predictions = np.array(all_predictions)\n",
        "for i in range(all_predictions.shape[0]):\n",
        "    # all_predictions[i, [2*j for j in range(24)]] *= 1024\n",
        "    # all_predictions[i, [2*j + 1 for j in range(24)]] *= 1024\n",
        "    all_predictions[i, [j for j in range(48)]] *= 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpsFaRPWX-_8"
      },
      "source": [
        "# submit * 1024 해주어야함."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDwRtPQcJdEN"
      },
      "source": [
        "all_predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6mY1IBKwum"
      },
      "source": [
        "submit = pd.read_csv(SUB_DF)\n",
        "submit.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATsD8D6cWaZv"
      },
      "source": [
        "submit = pd.DataFrame(columns=submit.columns)\n",
        "submit['image'] = files\n",
        "submit.iloc[:, 1:] = all_predictions\n",
        "submit.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ybu5f_Is-X"
      },
      "source": [
        "# inversed = scaler.inverse_transform(submit.iloc[:,1:])\n",
        "# print(inversed[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PavTtvYHK4UY"
      },
      "source": [
        "# submit.iloc[:,1:] = inversed\n",
        "# submit.head(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PGqOOYdLe8c"
      },
      "source": [
        "submit[x_coor] = (submit[x_coor] + 426)\n",
        "submit[y_coor] = (submit[y_coor] + 56)\n",
        "# submit[x_coor] = (submit[x_coor] * 2 + 426)\n",
        "# submit[y_coor] = (submit[y_coor] * 2 + 56)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5b94V4_LtMa"
      },
      "source": [
        "submit.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVq8i7MbLug0"
      },
      "source": [
        "submit.to_csv('/content/gdrive/MyDrive/dacon-motion/submit_hr_0326.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo7BLgnFL06g"
      },
      "source": [
        "import glob\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swNkhIZe-bP6"
      },
      "source": [
        "plt.figure(figsize=(40,20))\n",
        "count=1\n",
        "test_paths = glob.glob(os.path.join(PATH_TEST_DATASET,'*.jpg'))\n",
        "\n",
        "for i in np.random.randint(0,len(test_paths),5):\n",
        "    \n",
        "    plt.subplot(5,1, count)\n",
        "    \n",
        "    img_sample_path = test_paths[i]\n",
        "    img_name = img_sample_path.split('/')[6]\n",
        "    img = Image.open(img_sample_path)\n",
        "    img_np = np.array(img)\n",
        "    key = submit[submit['image'] == img_name].iloc[0, 1:49]\n",
        "\n",
        "    keypoint = submit.iloc[:,1:49] #위치키포인트 하나씩 확인\n",
        "\n",
        "    for j in range(0,len(keypoint.columns),2):\n",
        "        plt.plot(key[j], key[j+1],'rx')\n",
        "        plt.imshow(img_np)\n",
        "    \n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X5bSjwMU89E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKXyIL2FKcXA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuejLUykDqBb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}